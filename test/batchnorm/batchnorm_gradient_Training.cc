/*
 * Copyright Codeplay Software Ltd.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use these files except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// DO NOT MODIFY BY HAND
// This file was automatically generated by generate_batchnorm_tests.py.
// Results calculated using Tensorflow v2.8.0.

#include <gtest/gtest.h>

#include "sycldnn/data_format.h"

#include "sycldnn/batchnorm/direction.h"
#include "sycldnn/batchnorm/params.h"

#include "test/batchnorm/batchnorm_fixture.h"
#include "test/types/kernel_data_types.h"

#include <vector>

using namespace sycldnn;  // NOLINT(google-build-using-namespace)
template <typename DataType>
using BatchnormGradientTraining =
    BatchNormFixture<DataType, batchnorm::Gradient, batchnorm::Training>;
TYPED_TEST_CASE(BatchnormGradientTraining, types::GTestKernelDataTypes);
TYPED_TEST(BatchnormGradientTraining, 1x1x1x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {0.};
  const std::vector<DataType> mean = {1.};
  const std::vector<DataType> variance = {0.};
  const std::vector<DataType> grad_scale = {0.};
  const std::vector<DataType> grad_offset = {1.};
  const std::array<int, 4> in_shape = {{1, 1, 1, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 0.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x1x1x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 1., 2., 1.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {1., 2., 1., 2., 1.};
  const std::array<int, 4> in_shape = {{1, 1, 1, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 2.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x1x1x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::array<int, 4> in_shape = {{1, 1, 1, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x1x8x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743453617, -0.0004998495581151391, 0.0004998495581152501,
      0.0014995486743453617,  -0.0014995486743453617, -0.0004998495581151391,
      0.0004998495581152501,  0.0014995486743453617};
  const std::vector<DataType> mean = {2.5};
  const std::vector<DataType> variance = {1.25};
  const std::vector<DataType> grad_scale = {8.939799553006722};
  const std::vector<DataType> grad_offset = {20.};
  const std::array<int, 4> in_shape = {{1, 1, 8, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x1x8x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743453617, -0.0004998495581150281, 0.0004998495581152501,
      0.0014995486743453617,  -0.0014995486743453617, -0.0004998495581151391,
      0.0004998495581151391,  0.0014995486743453617,  -0.0014995486743453617,
      -0.0004998495581151391, 0.0004998495581152501,  0.0014995486743451397,
      -0.0014995486743453617, -0.0004998495581151391, 0.0004998495581152501,
      0.0014995486743453617,  -0.0014995486743451397, -0.0004998495581151391,
      0.0004998495581152501,  0.0014995486743453617,  -0.0014995486743453617,
      -0.0004998495581150281, 0.0004998495581152501,  0.0014995486743453617,
      -0.0014995486743453617, -0.0004998495581151391, 0.0004998495581151391,
      0.0014995486743453617,  -0.0014995486743453617, -0.0004998495581151391,
      0.0004998495581152501,  0.0014995486743451397,  -0.0014995486743453617,
      -0.0004998495581151391, 0.0004998495581152501,  0.0014995486743453617,
      -0.0014995486743451397, -0.0004998495581151391, 0.0004998495581152501,
      0.0014995486743453617};
  const std::vector<DataType> mean = {2.5, 2.5, 2.5, 2.5, 2.5};
  const std::vector<DataType> variance = {1.25, 1.25, 1.25, 1.25, 1.25};
  const std::vector<DataType> grad_scale = {
      8.939799553006722, 8.939799553006724, 8.939799553006722,
      8.939799553006722, 8.939799553006722};
  const std::vector<DataType> grad_offset = {20., 20., 20., 20., 20.};
  const std::array<int, 4> in_shape = {{1, 1, 8, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x1x8x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {8., 16., 24., 32.,
                                             8., 16., 24., 32.};
  const std::array<int, 4> in_shape = {{1, 1, 8, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x1x9x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0013328323660838505, -0.00033320809152112915, 0.0006664161830419252,
      0.0016660404576044245,  -0.0013328323660838505,  -0.00033320809152112915,
      0.0006664161830419252,  0.0016660404576044245,   -0.0013328323660838505};
  const std::vector<DataType> mean = {2.3333333333333335};
  const std::vector<DataType> variance = {1.3333333333333333};
  const std::vector<DataType> grad_scale = {10.38710869558636};
  const std::vector<DataType> grad_offset = {21.};
  const std::array<int, 4> in_shape = {{1, 1, 9, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x1x9x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0013328323660838507,  -0.00044436423004358037, 0.00044436423004369133,
      0.0013328323660838505,   -0.0013328323660838507,  -0.0003332080915210737,
      0.0005554552875541562,   0.0014441837476413168,   -0.0016660404576044245,
      -0.0003332080915210737,  0.0006664161830417034,   0.0015552748051517263,
      -0.0015552748051517263,  -0.0006664161830417032,  0.0006664161830417034,
      0.001666040457604647,    -0.0014441837476413168,  -0.0005554552875540452,
      0.0003332080915212956,   0.001666040457604647,    -0.0013328323660838507,
      -0.00044436423004358037, 0.00044436423004369133,  0.0013328323660838505,
      -0.0013328323660838507,  -0.0003332080915210737,  0.0005554552875541562,
      0.0014441837476413168,   -0.0016660404576044245,  -0.0003332080915210737,
      0.0006664161830417034,   0.0015552748051517263,   -0.0015552748051517263,
      -0.0006664161830417032,  0.0006664161830417034,   0.001666040457604647,
      -0.0014441837476413168,  -0.0005554552875540452,  0.0003332080915212956,
      0.001666040457604647,    -0.0013328323660838507,  -0.00044436423004358037,
      0.00044436423004369133,  0.0013328323660838505,   -0.0013328323660838507};
  const std::vector<DataType> mean = {2.3333333333333335, 2.4444444444444446,
                                      2.5555555555555554, 2.6666666666666665,
                                      2.3333333333333335};
  const std::vector<DataType> variance = {
      1.3333333333333335, 1.1358024691358024, 1.1358024691358024,
      1.3333333333333333, 1.3333333333333335};
  const std::vector<DataType> grad_scale = {
      10.38710869558636, 9.586866592892612, 9.586866592892612,
      10.38710869558636, 10.38710869558636};
  const std::vector<DataType> grad_offset = {21., 22., 23., 24., 21.};
  const std::array<int, 4> in_shape = {{1, 1, 9, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x1x9x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {9., 18., 27., 36.,
                                             9., 18., 27., 36.};
  const std::array<int, 4> in_shape = {{1, 1, 9, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x8x1x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743453617, -0.0004998495581151391, 0.0004998495581152501,
      0.0014995486743453617,  -0.0014995486743453617, -0.0004998495581151391,
      0.0004998495581152501,  0.0014995486743453617};
  const std::vector<DataType> mean = {2.5};
  const std::vector<DataType> variance = {1.25};
  const std::vector<DataType> grad_scale = {8.939799553006722};
  const std::vector<DataType> grad_offset = {20.};
  const std::array<int, 4> in_shape = {{1, 8, 1, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x8x1x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743453617, -0.0004998495581150281, 0.0004998495581152501,
      0.0014995486743453617,  -0.0014995486743453617, -0.0004998495581151391,
      0.0004998495581151391,  0.0014995486743453617,  -0.0014995486743453617,
      -0.0004998495581151391, 0.0004998495581152501,  0.0014995486743451397,
      -0.0014995486743453617, -0.0004998495581151391, 0.0004998495581152501,
      0.0014995486743453617,  -0.0014995486743451397, -0.0004998495581151391,
      0.0004998495581152501,  0.0014995486743453617,  -0.0014995486743453617,
      -0.0004998495581150281, 0.0004998495581152501,  0.0014995486743453617,
      -0.0014995486743453617, -0.0004998495581151391, 0.0004998495581151391,
      0.0014995486743453617,  -0.0014995486743453617, -0.0004998495581151391,
      0.0004998495581152501,  0.0014995486743451397,  -0.0014995486743453617,
      -0.0004998495581151391, 0.0004998495581152501,  0.0014995486743453617,
      -0.0014995486743451397, -0.0004998495581151391, 0.0004998495581152501,
      0.0014995486743453617};
  const std::vector<DataType> mean = {2.5, 2.5, 2.5, 2.5, 2.5};
  const std::vector<DataType> variance = {1.25, 1.25, 1.25, 1.25, 1.25};
  const std::vector<DataType> grad_scale = {
      8.939799553006722, 8.939799553006724, 8.939799553006722,
      8.939799553006722, 8.939799553006722};
  const std::vector<DataType> grad_offset = {20., 20., 20., 20., 20.};
  const std::array<int, 4> in_shape = {{1, 8, 1, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x8x1x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {8., 16., 24., 32.,
                                             8., 16., 24., 32.};
  const std::array<int, 4> in_shape = {{1, 8, 1, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x8x8x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.001499548674344474,  -0.0004998495581148061, 0.0004998495581149727,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581148061,
      0.0004998495581149727,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581148061, 0.0004998495581149727,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581148061, 0.0004998495581149727,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581148061,
      0.0004998495581149727,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581148061, 0.0004998495581149727,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581148061, 0.0004998495581149727,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581148061,
      0.0004998495581149727,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581148061, 0.0004998495581149727,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581148061, 0.0004998495581149727,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581148061,
      0.0004998495581149727,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581148061, 0.0004998495581149727,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581148061, 0.0004998495581149727,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581148061,
      0.0004998495581149727,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581148061, 0.0004998495581149727,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581148061, 0.0004998495581149727,
      0.001499548674344474};
  const std::vector<DataType> mean = {2.5};
  const std::vector<DataType> variance = {1.25};
  const std::vector<DataType> grad_scale = {71.5183964240538};
  const std::vector<DataType> grad_offset = {160.};
  const std::array<int, 4> in_shape = {{1, 8, 8, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x8x8x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743435862, -0.0004998495581144178, 0.0004998495581146397,
      0.0014995486743433642,  -0.0014995486743435862, -0.0004998495581145288,
      0.0004998495581145843,  0.0014995486743435862,  -0.0014995486743433642,
      -0.0004998495581145288, 0.0004998495581146397,  0.0014995486743433642,
      -0.0014995486743435862, -0.0004998495581144178, 0.0004998495581146397,
      0.0014995486743435862,  -0.0014995486743433642, -0.0004998495581145288,
      0.0004998495581145843,  0.0014995486743435862,  -0.0014995486743435862,
      -0.0004998495581144178, 0.0004998495581146397,  0.0014995486743433642,
      -0.0014995486743435862, -0.0004998495581145288, 0.0004998495581145843,
      0.0014995486743435862,  -0.0014995486743433642, -0.0004998495581145288,
      0.0004998495581146397,  0.0014995486743433642,  -0.0014995486743435862,
      -0.0004998495581144178, 0.0004998495581146397,  0.0014995486743435862,
      -0.0014995486743433642, -0.0004998495581145288, 0.0004998495581145843,
      0.0014995486743435862,  -0.0014995486743435862, -0.0004998495581144178,
      0.0004998495581146397,  0.0014995486743433642,  -0.0014995486743435862,
      -0.0004998495581145288, 0.0004998495581145843,  0.0014995486743435862,
      -0.0014995486743433642, -0.0004998495581145288, 0.0004998495581146397,
      0.0014995486743433642,  -0.0014995486743435862, -0.0004998495581144178,
      0.0004998495581146397,  0.0014995486743435862,  -0.0014995486743433642,
      -0.0004998495581145288, 0.0004998495581145843,  0.0014995486743435862,
      -0.0014995486743435862, -0.0004998495581144178, 0.0004998495581146397,
      0.0014995486743433642,  -0.0014995486743435862, -0.0004998495581145288,
      0.0004998495581145843,  0.0014995486743435862,  -0.0014995486743433642,
      -0.0004998495581145288, 0.0004998495581146397,  0.0014995486743433642,
      -0.0014995486743435862, -0.0004998495581144178, 0.0004998495581146397,
      0.0014995486743435862,  -0.0014995486743433642, -0.0004998495581145288,
      0.0004998495581145843,  0.0014995486743435862,  -0.0014995486743435862,
      -0.0004998495581144178, 0.0004998495581146397,  0.0014995486743433642,
      -0.0014995486743435862, -0.0004998495581145288, 0.0004998495581145843,
      0.0014995486743435862,  -0.0014995486743433642, -0.0004998495581145288,
      0.0004998495581146397,  0.0014995486743433642,  -0.0014995486743435862,
      -0.0004998495581144178, 0.0004998495581146397,  0.0014995486743435862,
      -0.0014995486743433642, -0.0004998495581145288, 0.0004998495581145843,
      0.0014995486743435862,  -0.0014995486743435862, -0.0004998495581144178,
      0.0004998495581146397,  0.0014995486743433642,  -0.0014995486743435862,
      -0.0004998495581145288, 0.0004998495581145843,  0.0014995486743435862,
      -0.0014995486743433642, -0.0004998495581145288, 0.0004998495581146397,
      0.0014995486743433642,  -0.0014995486743435862, -0.0004998495581144178,
      0.0004998495581146397,  0.0014995486743435862,  -0.0014995486743433642,
      -0.0004998495581145288, 0.0004998495581145843,  0.0014995486743435862,
      -0.0014995486743435862, -0.0004998495581144178, 0.0004998495581146397,
      0.0014995486743433642,  -0.0014995486743435862, -0.0004998495581145288,
      0.0004998495581145843,  0.0014995486743435862,  -0.0014995486743433642,
      -0.0004998495581145288, 0.0004998495581146397,  0.0014995486743433642,
      -0.0014995486743435862, -0.0004998495581144178, 0.0004998495581146397,
      0.0014995486743435862,  -0.0014995486743433642, -0.0004998495581145288,
      0.0004998495581145843,  0.0014995486743435862,  -0.0014995486743435862,
      -0.0004998495581144178, 0.0004998495581146397,  0.0014995486743433642,
      -0.0014995486743435862, -0.0004998495581145288, 0.0004998495581145843,
      0.0014995486743435862,  -0.0014995486743433642, -0.0004998495581145288,
      0.0004998495581146397,  0.0014995486743433642,  -0.0014995486743435862,
      -0.0004998495581144178, 0.0004998495581146397,  0.0014995486743435862,
      -0.0014995486743433642, -0.0004998495581145288, 0.0004998495581145843,
      0.0014995486743435862,  -0.0014995486743435862, -0.0004998495581144178,
      0.0004998495581146397,  0.0014995486743433642,  -0.0014995486743435862,
      -0.0004998495581145288, 0.0004998495581145843,  0.0014995486743435862,
      -0.0014995486743433642, -0.0004998495581145288, 0.0004998495581146397,
      0.0014995486743433642,  -0.0014995486743435862, -0.0004998495581144178,
      0.0004998495581146397,  0.0014995486743435862,  -0.0014995486743433642,
      -0.0004998495581145288, 0.0004998495581145843,  0.0014995486743435862,
      -0.0014995486743435862, -0.0004998495581144178, 0.0004998495581146397,
      0.0014995486743433642,  -0.0014995486743435862, -0.0004998495581145288,
      0.0004998495581145843,  0.0014995486743435862,  -0.0014995486743433642,
      -0.0004998495581145288, 0.0004998495581146397,  0.0014995486743433642,
      -0.0014995486743435862, -0.0004998495581144178, 0.0004998495581146397,
      0.0014995486743435862,  -0.0014995486743433642, -0.0004998495581145288,
      0.0004998495581145843,  0.0014995486743435862,  -0.0014995486743435862,
      -0.0004998495581144178, 0.0004998495581146397,  0.0014995486743433642,
      -0.0014995486743435862, -0.0004998495581145288, 0.0004998495581145843,
      0.0014995486743435862,  -0.0014995486743433642, -0.0004998495581145288,
      0.0004998495581146397,  0.0014995486743433642,  -0.0014995486743435862,
      -0.0004998495581144178, 0.0004998495581146397,  0.0014995486743435862,
      -0.0014995486743433642, -0.0004998495581145288, 0.0004998495581145843,
      0.0014995486743435862,  -0.0014995486743435862, -0.0004998495581144178,
      0.0004998495581146397,  0.0014995486743433642,  -0.0014995486743435862,
      -0.0004998495581145288, 0.0004998495581145843,  0.0014995486743435862,
      -0.0014995486743433642, -0.0004998495581145288, 0.0004998495581146397,
      0.0014995486743433642,  -0.0014995486743435862, -0.0004998495581144178,
      0.0004998495581146397,  0.0014995486743435862,  -0.0014995486743433642,
      -0.0004998495581145288, 0.0004998495581145843,  0.0014995486743435862,
      -0.0014995486743435862, -0.0004998495581144178, 0.0004998495581146397,
      0.0014995486743433642,  -0.0014995486743435862, -0.0004998495581145288,
      0.0004998495581145843,  0.0014995486743435862,  -0.0014995486743433642,
      -0.0004998495581145288, 0.0004998495581146397,  0.0014995486743433642,
      -0.0014995486743435862, -0.0004998495581144178, 0.0004998495581146397,
      0.0014995486743435862,  -0.0014995486743433642, -0.0004998495581145288,
      0.0004998495581145843,  0.0014995486743435862,  -0.0014995486743435862,
      -0.0004998495581144178, 0.0004998495581146397,  0.0014995486743433642,
      -0.0014995486743435862, -0.0004998495581145288, 0.0004998495581145843,
      0.0014995486743435862,  -0.0014995486743433642, -0.0004998495581145288,
      0.0004998495581146397,  0.0014995486743433642,  -0.0014995486743435862,
      -0.0004998495581144178, 0.0004998495581146397,  0.0014995486743435862,
      -0.0014995486743433642, -0.0004998495581145288, 0.0004998495581145843,
      0.0014995486743435862,  -0.0014995486743435862, -0.0004998495581144178,
      0.0004998495581146397,  0.0014995486743433642,  -0.0014995486743435862,
      -0.0004998495581145288, 0.0004998495581145843,  0.0014995486743435862,
      -0.0014995486743433642, -0.0004998495581145288, 0.0004998495581146397,
      0.0014995486743433642,  -0.0014995486743435862, -0.0004998495581144178,
      0.0004998495581146397,  0.0014995486743435862,  -0.0014995486743433642,
      -0.0004998495581145288, 0.0004998495581145843,  0.0014995486743435862,
      -0.0014995486743435862, -0.0004998495581144178, 0.0004998495581146397,
      0.0014995486743433642,  -0.0014995486743435862, -0.0004998495581145288,
      0.0004998495581145843,  0.0014995486743435862,  -0.0014995486743433642,
      -0.0004998495581145288, 0.0004998495581146397,  0.0014995486743433642,
      -0.0014995486743435862, -0.0004998495581144178, 0.0004998495581146397,
      0.0014995486743435862,  -0.0014995486743433642, -0.0004998495581145288,
      0.0004998495581145843,  0.0014995486743435862};
  const std::vector<DataType> mean = {2.5, 2.5, 2.5, 2.5, 2.5};
  const std::vector<DataType> variance = {1.25, 1.25, 1.25, 1.25, 1.25};
  const std::vector<DataType> grad_scale = {
      71.51839642405385, 71.51839642405385, 71.51839642405385,
      71.51839642405385, 71.51839642405385};
  const std::vector<DataType> grad_offset = {160., 160., 160., 160., 160.};
  const std::array<int, 4> in_shape = {{1, 8, 8, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x8x8x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {64., 128., 192., 256.,
                                             64., 128., 192., 256.};
  const std::array<int, 4> in_shape = {{1, 8, 8, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x8x9x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.001499548674344474,  -0.0004998495581147506, 0.0004998495581150282,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581147506,
      0.0004998495581150282,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581147506, 0.0004998495581150282,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581147506, 0.0004998495581150282,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581147506,
      0.0004998495581150282,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581147506, 0.0004998495581150282,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581147506, 0.0004998495581150282,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581147506,
      0.0004998495581150282,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581147506, 0.0004998495581150282,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581147506, 0.0004998495581150282,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581147506,
      0.0004998495581150282,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581147506, 0.0004998495581150282,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581147506, 0.0004998495581150282,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581147506,
      0.0004998495581150282,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581147506, 0.0004998495581150282,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581147506, 0.0004998495581150282,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581147506,
      0.0004998495581150282,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581147506, 0.0004998495581150282,  0.001499548674344474};
  const std::vector<DataType> mean = {2.5};
  const std::vector<DataType> variance = {1.25};
  const std::vector<DataType> grad_scale = {80.45819597706054};
  const std::vector<DataType> grad_offset = {180.};
  const std::array<int, 4> in_shape = {{1, 8, 9, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x8x9x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252};
  const std::vector<DataType> mean = {2.5, 2.5, 2.5, 2.5, 2.5};
  const std::vector<DataType> variance = {1.25, 1.25, 1.25, 1.25, 1.25};
  const std::vector<DataType> grad_scale = {
      80.45819597706056, 80.45819597706056, 80.45819597706056,
      80.45819597706056, 80.45819597706056};
  const std::vector<DataType> grad_offset = {180., 180., 180., 180., 180.};
  const std::array<int, 4> in_shape = {{1, 8, 9, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x8x9x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {72., 144., 216., 288.,
                                             72., 144., 216., 288.};
  const std::array<int, 4> in_shape = {{1, 8, 9, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x9x1x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0013328323660838505, -0.00033320809152112915, 0.0006664161830419252,
      0.0016660404576044245,  -0.0013328323660838505,  -0.00033320809152112915,
      0.0006664161830419252,  0.0016660404576044245,   -0.0013328323660838505};
  const std::vector<DataType> mean = {2.3333333333333335};
  const std::vector<DataType> variance = {1.3333333333333333};
  const std::vector<DataType> grad_scale = {10.38710869558636};
  const std::vector<DataType> grad_offset = {21.};
  const std::array<int, 4> in_shape = {{1, 9, 1, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x9x1x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0013328323660838507,  -0.00044436423004358037, 0.00044436423004369133,
      0.0013328323660838505,   -0.0013328323660838507,  -0.0003332080915210737,
      0.0005554552875541562,   0.0014441837476413168,   -0.0016660404576044245,
      -0.0003332080915210737,  0.0006664161830417034,   0.0015552748051517263,
      -0.0015552748051517263,  -0.0006664161830417032,  0.0006664161830417034,
      0.001666040457604647,    -0.0014441837476413168,  -0.0005554552875540452,
      0.0003332080915212956,   0.001666040457604647,    -0.0013328323660838507,
      -0.00044436423004358037, 0.00044436423004369133,  0.0013328323660838505,
      -0.0013328323660838507,  -0.0003332080915210737,  0.0005554552875541562,
      0.0014441837476413168,   -0.0016660404576044245,  -0.0003332080915210737,
      0.0006664161830417034,   0.0015552748051517263,   -0.0015552748051517263,
      -0.0006664161830417032,  0.0006664161830417034,   0.001666040457604647,
      -0.0014441837476413168,  -0.0005554552875540452,  0.0003332080915212956,
      0.001666040457604647,    -0.0013328323660838507,  -0.00044436423004358037,
      0.00044436423004369133,  0.0013328323660838505,   -0.0013328323660838507};
  const std::vector<DataType> mean = {2.3333333333333335, 2.4444444444444446,
                                      2.5555555555555554, 2.6666666666666665,
                                      2.3333333333333335};
  const std::vector<DataType> variance = {
      1.3333333333333335, 1.1358024691358024, 1.1358024691358024,
      1.3333333333333333, 1.3333333333333335};
  const std::vector<DataType> grad_scale = {
      10.38710869558636, 9.586866592892612, 9.586866592892612,
      10.38710869558636, 10.38710869558636};
  const std::vector<DataType> grad_offset = {21., 22., 23., 24., 21.};
  const std::array<int, 4> in_shape = {{1, 9, 1, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x9x1x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {9., 18., 27., 36.,
                                             9., 18., 27., 36.};
  const std::array<int, 4> in_shape = {{1, 9, 1, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x9x8x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.001499548674344474,  -0.0004998495581147506, 0.0004998495581150282,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581147506,
      0.0004998495581150282,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581147506, 0.0004998495581150282,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581147506, 0.0004998495581150282,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581147506,
      0.0004998495581150282,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581147506, 0.0004998495581150282,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581147506, 0.0004998495581150282,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581147506,
      0.0004998495581150282,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581147506, 0.0004998495581150282,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581147506, 0.0004998495581150282,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581147506,
      0.0004998495581150282,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581147506, 0.0004998495581150282,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581147506, 0.0004998495581150282,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581147506,
      0.0004998495581150282,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581147506, 0.0004998495581150282,  0.001499548674344474,
      -0.001499548674344474,  -0.0004998495581147506, 0.0004998495581150282,
      0.001499548674344474,   -0.001499548674344474,  -0.0004998495581147506,
      0.0004998495581150282,  0.001499548674344474,   -0.001499548674344474,
      -0.0004998495581147506, 0.0004998495581150282,  0.001499548674344474};
  const std::vector<DataType> mean = {2.5};
  const std::vector<DataType> variance = {1.25};
  const std::vector<DataType> grad_scale = {80.45819597706054};
  const std::vector<DataType> grad_offset = {180.};
  const std::array<int, 4> in_shape = {{1, 9, 8, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x9x8x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252,
      -0.001499548674344252,  -0.0004998495581147506, 0.0004998495581148616,
      0.001499548674344252,   -0.001499548674344252,  -0.0004998495581147506,
      0.0004998495581148616,  0.001499548674344252,   -0.001499548674344252,
      -0.0004998495581147506, 0.0004998495581148616,  0.001499548674344252};
  const std::vector<DataType> mean = {2.5, 2.5, 2.5, 2.5, 2.5};
  const std::vector<DataType> variance = {1.25, 1.25, 1.25, 1.25, 1.25};
  const std::vector<DataType> grad_scale = {
      80.45819597706056, 80.45819597706056, 80.45819597706056,
      80.45819597706056, 80.45819597706056};
  const std::vector<DataType> grad_offset = {180., 180., 180., 180., 180.};
  const std::array<int, 4> in_shape = {{1, 9, 8, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x9x8x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {72., 144., 216., 288.,
                                             72., 144., 216., 288.};
  const std::array<int, 4> in_shape = {{1, 9, 8, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x9x9x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014810188550528375,  -0.00048133112789198904, 0.0005183565992689705,
      0.001518044326429264,    -0.0014810188550528375,  -0.00048133112789198904,
      0.0005183565992689705,   0.001518044326429264,    -0.0014810188550528375,
      -0.00048133112789198904, 0.0005183565992689705,   0.001518044326429264,
      -0.0014810188550528375,  -0.00048133112789198904, 0.0005183565992689705,
      0.001518044326429264,    -0.0014810188550528375,  -0.00048133112789198904,
      0.0005183565992689705,   0.001518044326429264,    -0.0014810188550528375,
      -0.00048133112789198904, 0.0005183565992689705,   0.001518044326429264,
      -0.0014810188550528375,  -0.00048133112789198904, 0.0005183565992689705,
      0.001518044326429264,    -0.0014810188550528375,  -0.00048133112789198904,
      0.0005183565992689705,   0.001518044326429264,    -0.0014810188550528375,
      -0.00048133112789198904, 0.0005183565992689705,   0.001518044326429264,
      -0.0014810188550528375,  -0.00048133112789198904, 0.0005183565992689705,
      0.001518044326429264,    -0.0014810188550528375,  -0.00048133112789198904,
      0.0005183565992689705,   0.001518044326429264,    -0.0014810188550528375,
      -0.00048133112789198904, 0.0005183565992689705,   0.001518044326429264,
      -0.0014810188550528375,  -0.00048133112789198904, 0.0005183565992689705,
      0.001518044326429264,    -0.0014810188550528375,  -0.00048133112789198904,
      0.0005183565992689705,   0.001518044326429264,    -0.0014810188550528375,
      -0.00048133112789198904, 0.0005183565992689705,   0.001518044326429264,
      -0.0014810188550528375,  -0.00048133112789198904, 0.0005183565992689705,
      0.001518044326429264,    -0.0014810188550528375,  -0.00048133112789198904,
      0.0005183565992689705,   0.001518044326429264,    -0.0014810188550528375,
      -0.00048133112789198904, 0.0005183565992689705,   0.001518044326429264,
      -0.0014810188550528375,  -0.00048133112789198904, 0.0005183565992689705,
      0.001518044326429264,    -0.0014810188550528375,  -0.00048133112789198904,
      0.0005183565992689705,   0.001518044326429264,    -0.0014810188550528375};
  const std::vector<DataType> mean = {2.4814814814814814};
  const std::vector<DataType> variance = {1.262002743484225};
  const std::vector<DataType> grad_scale = {90.94900617288678};
  const std::vector<DataType> grad_offset = {201.};
  const std::array<int, 4> in_shape = {{1, 9, 9, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x9x9x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014810188550523934,  -0.0004936844930588963,  0.0004936844930583413,
      0.001481018855051506,    -0.0014810188550508402,  -0.0004813311278920445,
      0.0005060266053854382,   0.0014933955915016766,   -0.0015180443264281543,
      -0.00048133112789154527, 0.0005183565992686374,   0.0015057377038296617,
      -0.0015057377038278856,  -0.0005183565992681936,  0.0005183565992678607,
      0.0015180443264290419,   -0.001493395591503453,   -0.0005060266053848831,
      0.0004813311278917672,   0.0015180443264274886,   -0.0014810188550523934,
      -0.0004936844930588963,  0.0004936844930583413,   0.001481018855051506,
      -0.0014810188550508402,  -0.0004813311278920445,  0.0005060266053854382,
      0.0014933955915016766,   -0.0015180443264281543,  -0.00048133112789154527,
      0.0005183565992686374,   0.0015057377038296617,   -0.0015057377038278856,
      -0.0005183565992681936,  0.0005183565992678607,   0.0015180443264290419,
      -0.001493395591503453,   -0.0005060266053848831,  0.0004813311278917672,
      0.0015180443264274886,   -0.0014810188550523934,  -0.0004936844930588963,
      0.0004936844930583413,   0.001481018855051506,    -0.0014810188550508402,
      -0.0004813311278920445,  0.0005060266053854382,   0.0014933955915016766,
      -0.0015180443264281543,  -0.00048133112789154527, 0.0005183565992686374,
      0.0015057377038296617,   -0.0015057377038278856,  -0.0005183565992681936,
      0.0005183565992678607,   0.0015180443264290419,   -0.001493395591503453,
      -0.0005060266053848831,  0.0004813311278917672,   0.0015180443264274886,
      -0.0014810188550523934,  -0.0004936844930588963,  0.0004936844930583413,
      0.001481018855051506,    -0.0014810188550508402,  -0.0004813311278920445,
      0.0005060266053854382,   0.0014933955915016766,   -0.0015180443264281543,
      -0.00048133112789154527, 0.0005183565992686374,   0.0015057377038296617,
      -0.0015057377038278856,  -0.0005183565992681936,  0.0005183565992678607,
      0.0015180443264290419,   -0.001493395591503453,   -0.0005060266053848831,
      0.0004813311278917672,   0.0015180443264274886,   -0.0014810188550523934,
      -0.0004936844930588963,  0.0004936844930583413,   0.001481018855051506,
      -0.0014810188550508402,  -0.0004813311278920445,  0.0005060266053854382,
      0.0014933955915016766,   -0.0015180443264281543,  -0.00048133112789154527,
      0.0005183565992686374,   0.0015057377038296617,   -0.0015057377038278856,
      -0.0005183565992681936,  0.0005183565992678607,   0.0015180443264290419,
      -0.001493395591503453,   -0.0005060266053848831,  0.0004813311278917672,
      0.0015180443264274886,   -0.0014810188550523934,  -0.0004936844930588963,
      0.0004936844930583413,   0.001481018855051506,    -0.0014810188550508402,
      -0.0004813311278920445,  0.0005060266053854382,   0.0014933955915016766,
      -0.0015180443264281543,  -0.00048133112789154527, 0.0005183565992686374,
      0.0015057377038296617,   -0.0015057377038278856,  -0.0005183565992681936,
      0.0005183565992678607,   0.0015180443264290419,   -0.001493395591503453,
      -0.0005060266053848831,  0.0004813311278917672,   0.0015180443264274886,
      -0.0014810188550523934,  -0.0004936844930588963,  0.0004936844930583413,
      0.001481018855051506,    -0.0014810188550508402,  -0.0004813311278920445,
      0.0005060266053854382,   0.0014933955915016766,   -0.0015180443264281543,
      -0.00048133112789154527, 0.0005183565992686374,   0.0015057377038296617,
      -0.0015057377038278856,  -0.0005183565992681936,  0.0005183565992678607,
      0.0015180443264290419,   -0.001493395591503453,   -0.0005060266053848831,
      0.0004813311278917672,   0.0015180443264274886,   -0.0014810188550523934,
      -0.0004936844930588963,  0.0004936844930583413,   0.001481018855051506,
      -0.0014810188550508402,  -0.0004813311278920445,  0.0005060266053854382,
      0.0014933955915016766,   -0.0015180443264281543,  -0.00048133112789154527,
      0.0005183565992686374,   0.0015057377038296617,   -0.0015057377038278856,
      -0.0005183565992681936,  0.0005183565992678607,   0.0015180443264290419,
      -0.001493395591503453,   -0.0005060266053848831,  0.0004813311278917672,
      0.0015180443264274886,   -0.0014810188550523934,  -0.0004936844930588963,
      0.0004936844930583413,   0.001481018855051506,    -0.0014810188550508402,
      -0.0004813311278920445,  0.0005060266053854382,   0.0014933955915016766,
      -0.0015180443264281543,  -0.00048133112789154527, 0.0005183565992686374,
      0.0015057377038296617,   -0.0015057377038278856,  -0.0005183565992681936,
      0.0005183565992678607,   0.0015180443264290419,   -0.001493395591503453,
      -0.0005060266053848831,  0.0004813311278917672,   0.0015180443264274886,
      -0.0014810188550523934,  -0.0004936844930588963,  0.0004936844930583413,
      0.001481018855051506,    -0.0014810188550508402,  -0.0004813311278920445,
      0.0005060266053854382,   0.0014933955915016766,   -0.0015180443264281543,
      -0.00048133112789154527, 0.0005183565992686374,   0.0015057377038296617,
      -0.0015057377038278856,  -0.0005183565992681936,  0.0005183565992678607,
      0.0015180443264290419,   -0.001493395591503453,   -0.0005060266053848831,
      0.0004813311278917672,   0.0015180443264274886,   -0.0014810188550523934,
      -0.0004936844930588963,  0.0004936844930583413,   0.001481018855051506,
      -0.0014810188550508402,  -0.0004813311278920445,  0.0005060266053854382,
      0.0014933955915016766,   -0.0015180443264281543,  -0.00048133112789154527,
      0.0005183565992686374,   0.0015057377038296617,   -0.0015057377038278856,
      -0.0005183565992681936,  0.0005183565992678607,   0.0015180443264290419,
      -0.001493395591503453,   -0.0005060266053848831,  0.0004813311278917672,
      0.0015180443264274886,   -0.0014810188550523934,  -0.0004936844930588963,
      0.0004936844930583413,   0.001481018855051506,    -0.0014810188550508402,
      -0.0004813311278920445,  0.0005060266053854382,   0.0014933955915016766,
      -0.0015180443264281543,  -0.00048133112789154527, 0.0005183565992686374,
      0.0015057377038296617,   -0.0015057377038278856,  -0.0005183565992681936,
      0.0005183565992678607,   0.0015180443264290419,   -0.001493395591503453,
      -0.0005060266053848831,  0.0004813311278917672,   0.0015180443264274886,
      -0.0014810188550523934,  -0.0004936844930588963,  0.0004936844930583413,
      0.001481018855051506,    -0.0014810188550508402,  -0.0004813311278920445,
      0.0005060266053854382,   0.0014933955915016766,   -0.0015180443264281543,
      -0.00048133112789154527, 0.0005183565992686374,   0.0015057377038296617,
      -0.0015057377038278856,  -0.0005183565992681936,  0.0005183565992678607,
      0.0015180443264290419,   -0.001493395591503453,   -0.0005060266053848831,
      0.0004813311278917672,   0.0015180443264274886,   -0.0014810188550523934,
      -0.0004936844930588963,  0.0004936844930583413,   0.001481018855051506,
      -0.0014810188550508402,  -0.0004813311278920445,  0.0005060266053854382,
      0.0014933955915016766,   -0.0015180443264281543,  -0.00048133112789154527,
      0.0005183565992686374,   0.0015057377038296617,   -0.0015057377038278856,
      -0.0005183565992681936,  0.0005183565992678607,   0.0015180443264290419,
      -0.001493395591503453,   -0.0005060266053848831,  0.0004813311278917672,
      0.0015180443264274886,   -0.0014810188550523934,  -0.0004936844930588963,
      0.0004936844930583413,   0.001481018855051506,    -0.0014810188550508402,
      -0.0004813311278920445,  0.0005060266053854382,   0.0014933955915016766,
      -0.0015180443264281543,  -0.00048133112789154527, 0.0005183565992686374,
      0.0015057377038296617,   -0.0015057377038278856,  -0.0005183565992681936,
      0.0005183565992678607,   0.0015180443264290419,   -0.001493395591503453,
      -0.0005060266053848831,  0.0004813311278917672,   0.0015180443264274886,
      -0.0014810188550523934,  -0.0004936844930588963,  0.0004936844930583413,
      0.001481018855051506,    -0.0014810188550508402,  -0.0004813311278920445,
      0.0005060266053854382,   0.0014933955915016766,   -0.0015180443264281543,
      -0.00048133112789154527, 0.0005183565992686374,   0.0015057377038296617,
      -0.0015057377038278856,  -0.0005183565992681936,  0.0005183565992678607,
      0.0015180443264290419,   -0.001493395591503453,   -0.0005060266053848831,
      0.0004813311278917672,   0.0015180443264274886,   -0.0014810188550523934,
      -0.0004936844930588963,  0.0004936844930583413,   0.001481018855051506,
      -0.0014810188550508402,  -0.0004813311278920445,  0.0005060266053854382,
      0.0014933955915016766,   -0.0015180443264281543,  -0.00048133112789154527,
      0.0005183565992686374,   0.0015057377038296617,   -0.0015057377038278856,
      -0.0005183565992681936,  0.0005183565992678607,   0.0015180443264290419,
      -0.001493395591503453,   -0.0005060266053848831,  0.0004813311278917672,
      0.0015180443264274886,   -0.0014810188550523934,  -0.0004936844930588963,
      0.0004936844930583413,   0.001481018855051506,    -0.0014810188550508402,
      -0.0004813311278920445,  0.0005060266053854382,   0.0014933955915016766,
      -0.0015180443264281543,  -0.00048133112789154527, 0.0005183565992686374,
      0.0015057377038296617,   -0.0015057377038278856,  -0.0005183565992681936,
      0.0005183565992678607,   0.0015180443264290419,   -0.001493395591503453,
      -0.0005060266053848831,  0.0004813311278917672,   0.0015180443264274886,
      -0.0014810188550523934,  -0.0004936844930588963,  0.0004936844930583413,
      0.001481018855051506,    -0.0014810188550508402,  -0.0004813311278920445,
      0.0005060266053854382,   0.0014933955915016766,   -0.0015180443264281543,
      -0.00048133112789154527, 0.0005183565992686374,   0.0015057377038296617,
      -0.0015057377038278856,  -0.0005183565992681936,  0.0005183565992678607,
      0.0015180443264290419,   -0.001493395591503453,   -0.0005060266053848831,
      0.0004813311278917672,   0.0015180443264274886,   -0.0014810188550523934,
      -0.0004936844930588963,  0.0004936844930583413,   0.001481018855051506,
      -0.0014810188550508402,  -0.0004813311278920445,  0.0005060266053854382,
      0.0014933955915016766,   -0.0015180443264281543,  -0.00048133112789154527,
      0.0005183565992686374,   0.0015057377038296617,   -0.0015057377038278856,
      -0.0005183565992681936,  0.0005183565992678607,   0.0015180443264290419,
      -0.001493395591503453,   -0.0005060266053848831,  0.0004813311278917672,
      0.0015180443264274886,   -0.0014810188550523934,  -0.0004936844930588963,
      0.0004936844930583413,   0.001481018855051506,    -0.0014810188550508402};
  const std::vector<DataType> mean = {2.4814814814814814, 2.493827160493827,
                                      2.506172839506173, 2.5185185185185186,
                                      2.4814814814814814};
  const std::vector<DataType> variance = {1.262002743484225, 1.237616217040085,
                                          1.237616217040085, 1.2620027434842251,
                                          1.262002743484226};
  const std::vector<DataType> grad_scale = {
      90.94900617288681, 90.0659845002303, 90.06598450023041, 90.94900617288684,
      90.94900617288685};
  const std::vector<DataType> grad_offset = {201., 202., 203., 204., 201.};
  const std::array<int, 4> in_shape = {{1, 9, 9, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 1x9x9x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {81., 162., 243., 324.,
                                             81., 162., 243., 324.};
  const std::array<int, 4> in_shape = {{1, 9, 9, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x1x1x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {0., 0., 0.};
  const std::vector<DataType> mean = {1.};
  const std::vector<DataType> variance = {0.};
  const std::vector<DataType> grad_scale = {0.};
  const std::vector<DataType> grad_offset = {3.};
  const std::array<int, 4> in_shape = {{3, 1, 1, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 1.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x1x1x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0003350808528454036, 0.0003350808528454592,  -0.0003350808528454036,
      0.0003350808528454592,  -0.0003350808528454036, 0.0006701617056910295,
      -0.0006701617056911408, 0.0006701617056910295,  -0.0006701617056911408,
      0.0006701617056910295,  -0.0003350808528454036, 0.0003350808528454592,
      -0.0003350808528454036, 0.0003350808528454592,  -0.0003350808528454036};
  const std::vector<DataType> mean = {1.3333333333333333, 1.6666666666666667,
                                      1.3333333333333333, 1.6666666666666667,
                                      1.3333333333333333};
  const std::vector<DataType> variance = {
      0.2222222222222222, 0.2222222222222222, 0.2222222222222222,
      0.2222222222222222, 0.2222222222222222};
  const std::vector<DataType> grad_scale = {
      1.4135038082776477, 1.4135038082776474, 1.4135038082776477,
      1.4135038082776474, 1.4135038082776477};
  const std::vector<DataType> grad_offset = {4., 5., 4., 5., 4.};
  const std::array<int, 4> in_shape = {{3, 1, 1, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 2.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x1x1x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {0., 0., 0., 0., 0., 0., 0., 0.,
                                          0., 0., 0., 0., 0., 0., 0., 0.,
                                          0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {3., 6., 9., 12., 3., 6., 9., 12.};
  const std::array<int, 4> in_shape = {{3, 1, 1, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x1x8x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743453617, -0.0004998495581151391, 0.0004998495581152501,
      0.0014995486743453617,  -0.0014995486743453617, -0.0004998495581151391,
      0.0004998495581152501,  0.0014995486743453617,  -0.0014995486743453617,
      -0.0004998495581151391, 0.0004998495581152501,  0.0014995486743453617,
      -0.0014995486743453617, -0.0004998495581151391, 0.0004998495581152501,
      0.0014995486743453617,  -0.0014995486743453617, -0.0004998495581151391,
      0.0004998495581152501,  0.0014995486743453617,  -0.0014995486743453617,
      -0.0004998495581151391, 0.0004998495581152501,  0.0014995486743453617};
  const std::vector<DataType> mean = {2.5};
  const std::vector<DataType> variance = {1.25};
  const std::vector<DataType> grad_scale = {26.819398659020166};
  const std::vector<DataType> grad_offset = {60.};
  const std::array<int, 4> in_shape = {{3, 1, 8, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x1x8x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743460274, -0.0004998495581152501, 0.000499849558115472,
      0.0014995486743460279,  -0.0014995486743460274, -0.0004998495581153055,
      0.0004998495581153611,  0.0014995486743460274,  -0.0014995486743460279,
      -0.0004998495581153055, 0.000499849558115472,   0.0014995486743458058,
      -0.0014995486743460274, -0.0004998495581153056, 0.000499849558115472,
      0.0014995486743460274,  -0.0014995486743458058, -0.0004998495581153055,
      0.0004998495581154722,  0.0014995486743460274,  -0.0014995486743460274,
      -0.0004998495581152501, 0.000499849558115472,   0.0014995486743460279,
      -0.0014995486743460274, -0.0004998495581153055, 0.0004998495581153611,
      0.0014995486743460274,  -0.0014995486743460279, -0.0004998495581153055,
      0.000499849558115472,   0.0014995486743458058,  -0.0014995486743460274,
      -0.0004998495581153056, 0.000499849558115472,   0.0014995486743460274,
      -0.0014995486743458058, -0.0004998495581153055, 0.0004998495581154722,
      0.0014995486743460274,  -0.0014995486743460274, -0.0004998495581152501,
      0.000499849558115472,   0.0014995486743460279,  -0.0014995486743460274,
      -0.0004998495581153055, 0.0004998495581153611,  0.0014995486743460274,
      -0.0014995486743460279, -0.0004998495581153055, 0.000499849558115472,
      0.0014995486743458058,  -0.0014995486743460274, -0.0004998495581153056,
      0.000499849558115472,   0.0014995486743460274,  -0.0014995486743458058,
      -0.0004998495581153055, 0.0004998495581154722,  0.0014995486743460274,
      -0.0014995486743460274, -0.0004998495581152501, 0.000499849558115472,
      0.0014995486743460279,  -0.0014995486743460274, -0.0004998495581153055,
      0.0004998495581153611,  0.0014995486743460274,  -0.0014995486743460279,
      -0.0004998495581153055, 0.000499849558115472,   0.0014995486743458058,
      -0.0014995486743460274, -0.0004998495581153056, 0.000499849558115472,
      0.0014995486743460274,  -0.0014995486743458058, -0.0004998495581153055,
      0.0004998495581154722,  0.0014995486743460274,  -0.0014995486743460274,
      -0.0004998495581152501, 0.000499849558115472,   0.0014995486743460279,
      -0.0014995486743460274, -0.0004998495581153055, 0.0004998495581153611,
      0.0014995486743460274,  -0.0014995486743460279, -0.0004998495581153055,
      0.000499849558115472,   0.0014995486743458058,  -0.0014995486743460274,
      -0.0004998495581153056, 0.000499849558115472,   0.0014995486743460274,
      -0.0014995486743458058, -0.0004998495581153055, 0.0004998495581154722,
      0.0014995486743460274,  -0.0014995486743460274, -0.0004998495581152501,
      0.000499849558115472,   0.0014995486743460279,  -0.0014995486743460274,
      -0.0004998495581153055, 0.0004998495581153611,  0.0014995486743460274,
      -0.0014995486743460279, -0.0004998495581153055, 0.000499849558115472,
      0.0014995486743458058,  -0.0014995486743460274, -0.0004998495581153056,
      0.000499849558115472,   0.0014995486743460274,  -0.0014995486743458058,
      -0.0004998495581153055, 0.0004998495581154722,  0.0014995486743460274};
  const std::vector<DataType> mean = {2.5, 2.5, 2.5, 2.5, 2.5};
  const std::vector<DataType> variance = {1.25, 1.25, 1.25, 1.25, 1.25};
  const std::vector<DataType> grad_scale = {
      26.81939865902016, 26.81939865902017, 26.81939865902016,
      26.819398659020163, 26.81939865902016};
  const std::vector<DataType> grad_offset = {60., 60., 60., 60., 60.};
  const std::array<int, 4> in_shape = {{3, 1, 8, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x1x8x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {24., 48., 72., 96.,
                                             24., 48., 72., 96.};
  const std::array<int, 4> in_shape = {{3, 1, 8, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x1x9x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.001444067197286671,  -0.0004443283683961208, 0.0005554104604944294,
      0.0015551492893852016,  -0.001444067197286671,  -0.0004443283683961208,
      0.0005554104604944294,  0.0015551492893852016,  -0.001444067197286671,
      -0.0004443283683961208, 0.0005554104604944294,  0.0015551492893852016,
      -0.001444067197286671,  -0.0004443283683961208, 0.0005554104604944294,
      0.0015551492893852016,  -0.001444067197286671,  -0.0004443283683961208,
      0.0005554104604944294,  0.0015551492893852016,  -0.001444067197286671,
      -0.0004443283683961208, 0.0005554104604944294,  0.0015551492893852016,
      -0.001444067197286671,  -0.0004443283683961208, 0.0005554104604944294};
  const std::vector<DataType> mean = {2.4444444444444446};
  const std::vector<DataType> variance = {1.2098765432098768};
  const std::vector<DataType> grad_scale = {29.683634440130433};
  const std::vector<DataType> grad_offset = {66.};
  const std::array<int, 4> in_shape = {{3, 1, 9, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x1x9x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.001444067197286893,   -0.0005554104604946514,  0.00048132016905388827,
      0.0015180097639395105,   -0.001444067197286893,   -0.0004443283683961208,
      0.0004443283683960098,   0.0014809851355504766,   -0.0014809851355504766,
      -0.0004443283683961208,  0.0005554104604947624,   0.001444067197286893,
      -0.0015180097639395105,  -0.00048132016905388827, 0.0005554104604945404,
      0.0015551492893852016,   -0.0015551492893854236,  -0.0005183447974428111,
      0.0005183447974428111,   0.0015551492893854236,   -0.001444067197286893,
      -0.0005554104604946514,  0.00048132016905388827,  0.0015180097639395105,
      -0.001444067197286893,   -0.0004443283683961208,  0.0004443283683960098,
      0.0014809851355504766,   -0.0014809851355504766,  -0.0004443283683961208,
      0.0005554104604947624,   0.001444067197286893,    -0.0015180097639395105,
      -0.00048132016905388827, 0.0005554104604945404,   0.0015551492893852016,
      -0.0015551492893854236,  -0.0005183447974428111,  0.0005183447974428111,
      0.0015551492893854236,   -0.001444067197286893,   -0.0005554104604946514,
      0.00048132016905388827,  0.0015180097639395105,   -0.001444067197286893,
      -0.0004443283683961208,  0.0004443283683960098,   0.0014809851355504766,
      -0.0014809851355504766,  -0.0004443283683961208,  0.0005554104604947624,
      0.001444067197286893,    -0.0015180097639395105,  -0.00048132016905388827,
      0.0005554104604945404,   0.0015551492893852016,   -0.0015551492893854236,
      -0.0005183447974428111,  0.0005183447974428111,   0.0015551492893854236,
      -0.001444067197286893,   -0.0005554104604946514,  0.00048132016905388827,
      0.0015180097639395105,   -0.001444067197286893,   -0.0004443283683961208,
      0.0004443283683960098,   0.0014809851355504766,   -0.0014809851355504766,
      -0.0004443283683961208,  0.0005554104604947624,   0.001444067197286893,
      -0.0015180097639395105,  -0.00048132016905388827, 0.0005554104604945404,
      0.0015551492893852016,   -0.0015551492893854236,  -0.0005183447974428111,
      0.0005183447974428111,   0.0015551492893854236,   -0.001444067197286893,
      -0.0005554104604946514,  0.00048132016905388827,  0.0015180097639395105,
      -0.001444067197286893,   -0.0004443283683961208,  0.0004443283683960098,
      0.0014809851355504766,   -0.0014809851355504766,  -0.0004443283683961208,
      0.0005554104604947624,   0.001444067197286893,    -0.0015180097639395105,
      -0.00048132016905388827, 0.0005554104604945404,   0.0015551492893852016,
      -0.0015551492893854236,  -0.0005183447974428111,  0.0005183447974428111,
      0.0015551492893854236,   -0.001444067197286893,   -0.0005554104604946514,
      0.00048132016905388827,  0.0015180097639395105,   -0.001444067197286893,
      -0.0004443283683961208,  0.0004443283683960098,   0.0014809851355504766,
      -0.0014809851355504766,  -0.0004443283683961208,  0.0005554104604947624,
      0.001444067197286893,    -0.0015180097639395105,  -0.00048132016905388827,
      0.0005554104604945404,   0.0015551492893852016,   -0.0015551492893854236,
      -0.0005183447974428111,  0.0005183447974428111,   0.0015551492893854236,
      -0.001444067197286893,   -0.0005554104604946514,  0.00048132016905388827,
      0.0015180097639395105,   -0.001444067197286893,   -0.0004443283683961208,
      0.0004443283683960098,   0.0014809851355504766,   -0.0014809851355504766,
      -0.0004443283683961208,  0.0005554104604947624,   0.001444067197286893,
      -0.0015180097639395105,  -0.00048132016905388827, 0.0005554104604945404};
  const std::vector<DataType> mean = {2.4444444444444446, 2.5555555555555554,
                                      2.5185185185185186, 2.4814814814814814,
                                      2.4444444444444446};
  const std::vector<DataType> variance = {
      1.2098765432098764, 1.2098765432098768, 1.286694101508916,
      1.2866941015089164, 1.2098765432098768};
  const std::vector<DataType> grad_scale = {
      29.683634440130433, 29.683634440130433, 30.61147186136789,
      30.61147186136789, 29.68363444013043};
  const std::vector<DataType> grad_offset = {66., 69., 68., 67., 66.};
  const std::array<int, 4> in_shape = {{3, 1, 9, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x1x9x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {27., 54., 81., 108.,
                                             27., 54., 81., 108.};
  const std::array<int, 4> in_shape = {{3, 1, 9, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x8x1x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743453617, -0.0004998495581151391, 0.0004998495581152501,
      0.0014995486743453617,  -0.0014995486743453617, -0.0004998495581151391,
      0.0004998495581152501,  0.0014995486743453617,  -0.0014995486743453617,
      -0.0004998495581151391, 0.0004998495581152501,  0.0014995486743453617,
      -0.0014995486743453617, -0.0004998495581151391, 0.0004998495581152501,
      0.0014995486743453617,  -0.0014995486743453617, -0.0004998495581151391,
      0.0004998495581152501,  0.0014995486743453617,  -0.0014995486743453617,
      -0.0004998495581151391, 0.0004998495581152501,  0.0014995486743453617};
  const std::vector<DataType> mean = {2.5};
  const std::vector<DataType> variance = {1.25};
  const std::vector<DataType> grad_scale = {26.819398659020166};
  const std::vector<DataType> grad_offset = {60.};
  const std::array<int, 4> in_shape = {{3, 8, 1, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x8x1x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743460274, -0.0004998495581152501, 0.000499849558115472,
      0.0014995486743460279,  -0.0014995486743460274, -0.0004998495581153055,
      0.0004998495581153611,  0.0014995486743460274,  -0.0014995486743460279,
      -0.0004998495581153055, 0.000499849558115472,   0.0014995486743458058,
      -0.0014995486743460274, -0.0004998495581153056, 0.000499849558115472,
      0.0014995486743460274,  -0.0014995486743458058, -0.0004998495581153055,
      0.0004998495581154722,  0.0014995486743460274,  -0.0014995486743460274,
      -0.0004998495581152501, 0.000499849558115472,   0.0014995486743460279,
      -0.0014995486743460274, -0.0004998495581153055, 0.0004998495581153611,
      0.0014995486743460274,  -0.0014995486743460279, -0.0004998495581153055,
      0.000499849558115472,   0.0014995486743458058,  -0.0014995486743460274,
      -0.0004998495581153056, 0.000499849558115472,   0.0014995486743460274,
      -0.0014995486743458058, -0.0004998495581153055, 0.0004998495581154722,
      0.0014995486743460274,  -0.0014995486743460274, -0.0004998495581152501,
      0.000499849558115472,   0.0014995486743460279,  -0.0014995486743460274,
      -0.0004998495581153055, 0.0004998495581153611,  0.0014995486743460274,
      -0.0014995486743460279, -0.0004998495581153055, 0.000499849558115472,
      0.0014995486743458058,  -0.0014995486743460274, -0.0004998495581153056,
      0.000499849558115472,   0.0014995486743460274,  -0.0014995486743458058,
      -0.0004998495581153055, 0.0004998495581154722,  0.0014995486743460274,
      -0.0014995486743460274, -0.0004998495581152501, 0.000499849558115472,
      0.0014995486743460279,  -0.0014995486743460274, -0.0004998495581153055,
      0.0004998495581153611,  0.0014995486743460274,  -0.0014995486743460279,
      -0.0004998495581153055, 0.000499849558115472,   0.0014995486743458058,
      -0.0014995486743460274, -0.0004998495581153056, 0.000499849558115472,
      0.0014995486743460274,  -0.0014995486743458058, -0.0004998495581153055,
      0.0004998495581154722,  0.0014995486743460274,  -0.0014995486743460274,
      -0.0004998495581152501, 0.000499849558115472,   0.0014995486743460279,
      -0.0014995486743460274, -0.0004998495581153055, 0.0004998495581153611,
      0.0014995486743460274,  -0.0014995486743460279, -0.0004998495581153055,
      0.000499849558115472,   0.0014995486743458058,  -0.0014995486743460274,
      -0.0004998495581153056, 0.000499849558115472,   0.0014995486743460274,
      -0.0014995486743458058, -0.0004998495581153055, 0.0004998495581154722,
      0.0014995486743460274,  -0.0014995486743460274, -0.0004998495581152501,
      0.000499849558115472,   0.0014995486743460279,  -0.0014995486743460274,
      -0.0004998495581153055, 0.0004998495581153611,  0.0014995486743460274,
      -0.0014995486743460279, -0.0004998495581153055, 0.000499849558115472,
      0.0014995486743458058,  -0.0014995486743460274, -0.0004998495581153056,
      0.000499849558115472,   0.0014995486743460274,  -0.0014995486743458058,
      -0.0004998495581153055, 0.0004998495581154722,  0.0014995486743460274};
  const std::vector<DataType> mean = {2.5, 2.5, 2.5, 2.5, 2.5};
  const std::vector<DataType> variance = {1.25, 1.25, 1.25, 1.25, 1.25};
  const std::vector<DataType> grad_scale = {
      26.81939865902016, 26.81939865902017, 26.81939865902016,
      26.819398659020163, 26.81939865902016};
  const std::vector<DataType> grad_offset = {60., 60., 60., 60., 60.};
  const std::array<int, 4> in_shape = {{3, 8, 1, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x8x1x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {24., 48., 72., 96.,
                                             24., 48., 72., 96.};
  const std::array<int, 4> in_shape = {{3, 8, 1, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x8x8x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581149725, 0.0004998495581151946,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581149725,
      0.0004998495581151946,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581149725, 0.0004998495581151946,  0.0014995486743451397};
  const std::vector<DataType> mean = {2.5};
  const std::vector<DataType> variance = {1.25};
  const std::vector<DataType> grad_scale = {214.5551892721614};
  const std::vector<DataType> grad_offset = {480.};
  const std::array<int, 4> in_shape = {{3, 8, 8, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x8x8x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358,
      -0.0014995486743491358, -0.0004998495581163046, 0.0004998495581164711,
      0.0014995486743491358,  -0.0014995486743491358, -0.0004998495581163046,
      0.0004998495581164711,  0.0014995486743491358,  -0.0014995486743491358,
      -0.0004998495581163046, 0.0004998495581164711,  0.0014995486743491358};
  const std::vector<DataType> mean = {2.5, 2.5, 2.5, 2.5, 2.5};
  const std::vector<DataType> variance = {1.25, 1.25, 1.25, 1.25, 1.25};
  const std::vector<DataType> grad_scale = {
      214.5551892721609, 214.5551892721609, 214.5551892721609,
      214.5551892721609, 214.5551892721609};
  const std::vector<DataType> grad_offset = {480., 480., 480., 480., 480.};
  const std::array<int, 4> in_shape = {{3, 8, 8, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x8x8x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {192., 384., 576., 768.,
                                             192., 384., 576., 768.};
  const std::array<int, 4> in_shape = {{3, 8, 8, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x8x9x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397};
  const std::vector<DataType> mean = {2.5};
  const std::vector<DataType> variance = {1.25};
  const std::vector<DataType> grad_scale = {241.37458793118157};
  const std::vector<DataType> grad_offset = {540.};
  const std::array<int, 4> in_shape = {{3, 8, 9, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x8x9x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579};
  const std::vector<DataType> mean = {2.5, 2.5, 2.5, 2.5, 2.5};
  const std::vector<DataType> variance = {1.25, 1.25, 1.25, 1.25, 1.25};
  const std::vector<DataType> grad_scale = {
      241.37458793118094, 241.37458793118094, 241.37458793118094,
      241.37458793118094, 241.37458793118094};
  const std::vector<DataType> grad_offset = {540., 540., 540., 540., 540.};
  const std::array<int, 4> in_shape = {{3, 8, 9, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x8x9x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {216., 432., 648., 864.,
                                             216., 432., 648., 864.};
  const std::array<int, 4> in_shape = {{3, 8, 9, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x9x1x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.001444067197286671,  -0.0004443283683961208, 0.0005554104604944294,
      0.0015551492893852016,  -0.001444067197286671,  -0.0004443283683961208,
      0.0005554104604944294,  0.0015551492893852016,  -0.001444067197286671,
      -0.0004443283683961208, 0.0005554104604944294,  0.0015551492893852016,
      -0.001444067197286671,  -0.0004443283683961208, 0.0005554104604944294,
      0.0015551492893852016,  -0.001444067197286671,  -0.0004443283683961208,
      0.0005554104604944294,  0.0015551492893852016,  -0.001444067197286671,
      -0.0004443283683961208, 0.0005554104604944294,  0.0015551492893852016,
      -0.001444067197286671,  -0.0004443283683961208, 0.0005554104604944294};
  const std::vector<DataType> mean = {2.4444444444444446};
  const std::vector<DataType> variance = {1.2098765432098768};
  const std::vector<DataType> grad_scale = {29.683634440130433};
  const std::vector<DataType> grad_offset = {66.};
  const std::array<int, 4> in_shape = {{3, 9, 1, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x9x1x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.001444067197286893,   -0.0005554104604946514,  0.00048132016905388827,
      0.0015180097639395105,   -0.001444067197286893,   -0.0004443283683961208,
      0.0004443283683960098,   0.0014809851355504766,   -0.0014809851355504766,
      -0.0004443283683961208,  0.0005554104604947624,   0.001444067197286893,
      -0.0015180097639395105,  -0.00048132016905388827, 0.0005554104604945404,
      0.0015551492893852016,   -0.0015551492893854236,  -0.0005183447974428111,
      0.0005183447974428111,   0.0015551492893854236,   -0.001444067197286893,
      -0.0005554104604946514,  0.00048132016905388827,  0.0015180097639395105,
      -0.001444067197286893,   -0.0004443283683961208,  0.0004443283683960098,
      0.0014809851355504766,   -0.0014809851355504766,  -0.0004443283683961208,
      0.0005554104604947624,   0.001444067197286893,    -0.0015180097639395105,
      -0.00048132016905388827, 0.0005554104604945404,   0.0015551492893852016,
      -0.0015551492893854236,  -0.0005183447974428111,  0.0005183447974428111,
      0.0015551492893854236,   -0.001444067197286893,   -0.0005554104604946514,
      0.00048132016905388827,  0.0015180097639395105,   -0.001444067197286893,
      -0.0004443283683961208,  0.0004443283683960098,   0.0014809851355504766,
      -0.0014809851355504766,  -0.0004443283683961208,  0.0005554104604947624,
      0.001444067197286893,    -0.0015180097639395105,  -0.00048132016905388827,
      0.0005554104604945404,   0.0015551492893852016,   -0.0015551492893854236,
      -0.0005183447974428111,  0.0005183447974428111,   0.0015551492893854236,
      -0.001444067197286893,   -0.0005554104604946514,  0.00048132016905388827,
      0.0015180097639395105,   -0.001444067197286893,   -0.0004443283683961208,
      0.0004443283683960098,   0.0014809851355504766,   -0.0014809851355504766,
      -0.0004443283683961208,  0.0005554104604947624,   0.001444067197286893,
      -0.0015180097639395105,  -0.00048132016905388827, 0.0005554104604945404,
      0.0015551492893852016,   -0.0015551492893854236,  -0.0005183447974428111,
      0.0005183447974428111,   0.0015551492893854236,   -0.001444067197286893,
      -0.0005554104604946514,  0.00048132016905388827,  0.0015180097639395105,
      -0.001444067197286893,   -0.0004443283683961208,  0.0004443283683960098,
      0.0014809851355504766,   -0.0014809851355504766,  -0.0004443283683961208,
      0.0005554104604947624,   0.001444067197286893,    -0.0015180097639395105,
      -0.00048132016905388827, 0.0005554104604945404,   0.0015551492893852016,
      -0.0015551492893854236,  -0.0005183447974428111,  0.0005183447974428111,
      0.0015551492893854236,   -0.001444067197286893,   -0.0005554104604946514,
      0.00048132016905388827,  0.0015180097639395105,   -0.001444067197286893,
      -0.0004443283683961208,  0.0004443283683960098,   0.0014809851355504766,
      -0.0014809851355504766,  -0.0004443283683961208,  0.0005554104604947624,
      0.001444067197286893,    -0.0015180097639395105,  -0.00048132016905388827,
      0.0005554104604945404,   0.0015551492893852016,   -0.0015551492893854236,
      -0.0005183447974428111,  0.0005183447974428111,   0.0015551492893854236,
      -0.001444067197286893,   -0.0005554104604946514,  0.00048132016905388827,
      0.0015180097639395105,   -0.001444067197286893,   -0.0004443283683961208,
      0.0004443283683960098,   0.0014809851355504766,   -0.0014809851355504766,
      -0.0004443283683961208,  0.0005554104604947624,   0.001444067197286893,
      -0.0015180097639395105,  -0.00048132016905388827, 0.0005554104604945404};
  const std::vector<DataType> mean = {2.4444444444444446, 2.5555555555555554,
                                      2.5185185185185186, 2.4814814814814814,
                                      2.4444444444444446};
  const std::vector<DataType> variance = {
      1.2098765432098764, 1.2098765432098768, 1.286694101508916,
      1.2866941015089164, 1.2098765432098768};
  const std::vector<DataType> grad_scale = {
      29.683634440130433, 29.683634440130433, 30.61147186136789,
      30.61147186136789, 29.68363444013043};
  const std::vector<DataType> grad_offset = {66., 69., 68., 67., 66.};
  const std::array<int, 4> in_shape = {{3, 9, 1, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x9x1x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {27., 54., 81., 108.,
                                             27., 54., 81., 108.};
  const std::array<int, 4> in_shape = {{3, 9, 1, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x9x8x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397,
      -0.0014995486743451397, -0.0004998495581150281, 0.0004998495581151391,
      0.0014995486743451397,  -0.0014995486743451397, -0.0004998495581150281,
      0.0004998495581151391,  0.0014995486743451397,  -0.0014995486743451397,
      -0.0004998495581150281, 0.0004998495581151391,  0.0014995486743451397};
  const std::vector<DataType> mean = {2.5};
  const std::vector<DataType> variance = {1.25};
  const std::vector<DataType> grad_scale = {241.37458793118157};
  const std::vector<DataType> grad_offset = {540.};
  const std::array<int, 4> in_shape = {{3, 9, 8, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x9x8x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579,
      -0.0014995486743493579, -0.0004998495581164156, 0.0004998495581165266,
      0.0014995486743493579,  -0.0014995486743493579, -0.0004998495581164156,
      0.0004998495581165266,  0.0014995486743493579,  -0.0014995486743493579,
      -0.0004998495581164156, 0.0004998495581165266,  0.0014995486743493579};
  const std::vector<DataType> mean = {2.5, 2.5, 2.5, 2.5, 2.5};
  const std::vector<DataType> variance = {1.25, 1.25, 1.25, 1.25, 1.25};
  const std::vector<DataType> grad_scale = {
      241.37458793118094, 241.37458793118094, 241.37458793118094,
      241.37458793118094, 241.37458793118094};
  const std::vector<DataType> grad_offset = {540., 540., 540., 540., 540.};
  const std::array<int, 4> in_shape = {{3, 9, 8, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x9x8x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {216., 432., 648., 864.,
                                             216., 432., 648., 864.};
  const std::array<int, 4> in_shape = {{3, 9, 8, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x9x9x1) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248,
      0.0015057256693317969,  -0.0014933836556483635, -0.0004936805473216249,
      0.0005060225610052248,  0.0015057256693317969,  -0.0014933836556483635,
      -0.0004936805473216249, 0.0005060225610052248,  0.0015057256693317969,
      -0.0014933836556483635, -0.0004936805473216249, 0.0005060225610052248};
  const std::vector<DataType> mean = {2.493827160493827};
  const std::vector<DataType> variance = {1.2458466697149824};
  const std::vector<DataType> grad_scale = {271.09490692647796};
  const std::vector<DataType> grad_offset = {606.};
  const std::array<int, 4> in_shape = {{3, 9, 9, 1}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x9x9x5) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336,
      0.0015057256693400108,  -0.001505725669334238,  -0.0005019045814256645,
      0.0005019045814242214,  0.0015057256693393447,  -0.0014933836556565775,
      -0.0005060225610057794, 0.000497790609446318,   0.0015015997722939282,
      -0.0014933836556561334, -0.0004936805473245108, 0.0004936805473226792,
      0.0014974858003185226,  -0.0014974858003145262, -0.0004936805473243999,
      0.0005060225610076667,  0.0014933836556510268,  -0.0015015997722974805,
      -0.0004977906094451523, 0.0005060225610073336,  0.0015057256693400108,
      -0.001505725669334238,  -0.0005019045814256645, 0.0005019045814242214,
      0.0015057256693393447,  -0.0014933836556565775, -0.0005060225610057794,
      0.000497790609446318,   0.0015015997722939282,  -0.0014933836556561334,
      -0.0004936805473245108, 0.0004936805473226792,  0.0014974858003185226,
      -0.0014974858003145262, -0.0004936805473243999, 0.0005060225610076667,
      0.0014933836556510268,  -0.0015015997722974805, -0.0004977906094451523,
      0.0005060225610073336,  0.0015057256693400108,  -0.001505725669334238,
      -0.0005019045814256645, 0.0005019045814242214,  0.0015057256693393447,
      -0.0014933836556565775, -0.0005060225610057794, 0.000497790609446318,
      0.0015015997722939282,  -0.0014933836556561334, -0.0004936805473245108,
      0.0004936805473226792,  0.0014974858003185226,  -0.0014974858003145262,
      -0.0004936805473243999, 0.0005060225610076667,  0.0014933836556510268,
      -0.0015015997722974805, -0.0004977906094451523, 0.0005060225610073336};
  const std::vector<DataType> mean = {2.493827160493827, 2.506172839506173,
                                      2.5020576131687244, 2.4979423868312756,
                                      2.493827160493827};
  const std::vector<DataType> variance = {
      1.2458466697149835, 1.2458466697149835, 1.254110992565497,
      1.254110992565497, 1.2458466697149828};
  const std::vector<DataType> grad_scale = {
      271.09490692647677, 271.0949069264778, 271.9925753563653,
      271.99257535636605, 271.0949069264768};
  const std::vector<DataType> grad_offset = {606., 609., 608., 607., 606.};
  const std::array<int, 4> in_shape = {{3, 9, 9, 5}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
TYPED_TEST(BatchnormGradientTraining, 3x9x9x8) {
  using DataType = typename TestFixture::DataType;
  const std::vector<DataType> exp_grad = {
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> mean = {1., 2., 3., 4., 1., 2., 3., 4.};
  const std::vector<DataType> variance = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_scale = {0., 0., 0., 0., 0., 0., 0., 0.};
  const std::vector<DataType> grad_offset = {243., 486., 729., 972.,
                                             243., 486., 729., 972.};
  const std::array<int, 4> in_shape = {{3, 9, 9, 8}};
  const auto params = getBatchNormParams(in_shape, DataFormat::NHWC);
  const DataType max_input_val = 4.0;
  this->test_batchnorm(exp_grad, mean, variance, grad_scale, grad_offset,
                       params, max_input_val);
}
